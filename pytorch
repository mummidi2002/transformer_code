import torch
import torch.nn as nn

class SimpleTransformerEncoder(nn.Module):
    def __init__(self, d_model=128, num_heads=8, ff_hidden=512):
        super().__init__()
        
        # Multi-head self-attention
        self.mha = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)

        # Feed Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(d_model, ff_hidden),
            nn.ReLU(),
            nn.Linear(ff_hidden, d_model)
        )
        
        # Layer Normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):
        # x shape: (batch_size, seq_len, d_model)

        # ---- Multi-Head Attention ----
        attn_output, _ = self.mha(x, x, x)     # self-attention
        x = self.norm1(x + attn_output)         # Add & Norm

        # ---- Feed-Forward Network ----
        ffn_output = self.ffn(x)
        x = self.norm2(x + ffn_output)          # Add & Norm

        return x



# Testing the encoder block

if __name__ == "__main__":
    batch_size = 32
    seq_len = 10
    d_model = 128
    
    # Random input
    x = torch.randn(batch_size, seq_len, d_model)

    encoder = SimpleTransformerEncoder(d_model=128, num_heads=8)
    output = encoder(x)

    print("Input shape: ", x.shape)
    print("Output shape:", output.shape)
